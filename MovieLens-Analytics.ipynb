{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "81bc221d-de32-452b-9e34-a65ae080526e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "dbutils.widgets.removeAll()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "98228c88-3267-404c-bbea-21bfe5467c9d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# DEMO using taskvalues.get, but not recommended as we may use different name for tasks in different workflow.\n",
    "\n",
    "# read task values from downstream jobs\n",
    "# movies_path = dbutils.jobs.taskValues.get(taskKey = \"movies_to_silver\", key = \"movies_silver_path\", debugValue = \"movies-not-found\")\n",
    "\n",
    "# print (\"MOVIES PATH FROM PREVIOUS ONE \", movies_path)\n",
    "\n",
    "# BEST PRACTICE: use dynamic variables in the task parameters\n",
    "\n",
    "# add parameter with expression {{tasks.<task_name>.values.<value_name>}}.\n",
    "# like parameter_name = {{tasks.movies_to_silver.values.movies_silver_path}}\n",
    "# then use widgets.get to get the value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b6f33994-030f-4108-a8e8-cbdec1336cf8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "dbutils.widgets.text(\"ratingsPath\", \"abfss://silver@gksdatalake.dfs.core.windows.net/ratings/\")\n",
    "\n",
    "dbutils.widgets.text(\"moviesSilverPath\", \"abfss://silver@gksdatalake.dfs.core.windows.net/movies/\")\n",
    "\n",
    "dbutils.widgets.text(\"popularMoviesPath\", \"abfss://gold@gksdatalake.dfs.core.windows.net/popular-movies/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "86f17e4b-f29d-444f-b46c-c16681906eb6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# READ SILVER DATA WHICH IS PARQUET FORMAT\n",
    "# DATA ANALYTICS , ENRICHMENT ON SILVER DATA, NOT CSV/BRONZE\n",
    "# QUALITY OF DATA IMPROVED, LIKE RATING > 0.5\n",
    "# Find Top rated movies\n",
    "# At leasted rated by 100 users, avg rating should be 4 or above\n",
    "# MUST Modify two things, storage account name, and the key\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a07dd6fc-c498-4bda-bd28-6646f87fa551",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "abfss://silver@gksdatalake.dfs.core.windows.net/movies/\nabfss://silver@gksdatalake.dfs.core.windows.net/ratings/\n"
     ]
    }
   ],
   "source": [
    "MOVIES_PATH = dbutils.widgets.get(\"moviesSilverPath\")\n",
    "RATINGS_PATH= dbutils.widgets.get(\"ratingsPath\")\n",
    "print (MOVIES_PATH)\n",
    "print (RATINGS_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b2b7f5b0-a50e-459f-a432-760b65b5b901",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n |-- movieId: integer (nullable = true)\n |-- title: string (nullable = true)\n |-- genres: string (nullable = true)\n\n+-------+----------------------------------+-------------------------------------------+\n|movieId|title                             |genres                                     |\n+-------+----------------------------------+-------------------------------------------+\n|1      |Toy Story (1995)                  |Adventure|Animation|Children|Comedy|Fantasy|\n|2      |Jumanji (1995)                    |Adventure|Children|Fantasy                 |\n|3      |Grumpier Old Men (1995)           |Comedy|Romance                             |\n|4      |Waiting to Exhale (1995)          |Comedy|Drama|Romance                       |\n|5      |Father of the Bride Part II (1995)|Comedy                                     |\n+-------+----------------------------------+-------------------------------------------+\nonly showing top 5 rows\n\n"
     ]
    }
   ],
   "source": [
    "# PARQUET HAS SCHEMA AT END OF THE FILE\n",
    "# HERE IT IS NOT INFER DATA FOR SCHEMA\n",
    "movieDf = spark.read.parquet(MOVIES_PATH) \n",
    "movieDf.printSchema()\n",
    "movieDf.show(5, truncate = False) # does not truncate long strings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5cf67b84-485e-4f3f-8c2f-525310eee40c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n |-- userId: integer (nullable = true)\n |-- movieId: integer (nullable = true)\n |-- rating: double (nullable = true)\n |-- timestamp: long (nullable = true)\n\n+------+-------+------+---------+\n|userId|movieId|rating|timestamp|\n+------+-------+------+---------+\n|     1|      1|   4.0|964982703|\n|     1|      3|   4.0|964981247|\n|     1|      6|   4.0|964982224|\n|     1|     47|   5.0|964983815|\n|     1|     50|   5.0|964982931|\n+------+-------+------+---------+\nonly showing top 5 rows\n\n"
     ]
    }
   ],
   "source": [
    "ratingDf = spark.read.parquet(RATINGS_PATH)\n",
    "ratingDf.printSchema()\n",
    "ratingDf.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6401c4d9-f597-4650-95b2-75ed4a1536f1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hi col rating\nRATING\nColumn<'rating'>\n"
     ]
    }
   ],
   "source": [
    "# dont write this code , \n",
    "def col(name):\n",
    "    print (\"hi col\", name)\n",
    "    return name.upper()\n",
    "\n",
    "print (col(\"rating\")) # our col defined above\n",
    "\n",
    "# we have functions from std lib, 3rd party librares, pyspark etc, we import function etc, and use them\n",
    "\n",
    "from pyspark.sql.functions import col  # col from pyspark will override your col function\n",
    "\n",
    "# from pyspark.sql.functions import * # also override your functions\n",
    "\n",
    "print (col(\"rating\")) # this calls pyspark col"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "992084ba-8028-4fe0-be64-ce0b3bd69e82",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n |-- movieId: integer (nullable = true)\n |-- userCount: long (nullable = false)\n |-- avgRating: double (nullable = true)\n\n+-------+---------+------------------+\n|movieId|userCount|         avgRating|\n+-------+---------+------------------+\n|    356|      328| 4.175304878048781|\n|    318|      317| 4.429022082018927|\n|    296|      305| 4.221311475409836|\n|    593|      276| 4.201086956521739|\n|   2571|      273|  4.26007326007326|\n|    260|      250|             4.246|\n|    480|      237|3.7637130801687766|\n|    110|      236| 4.046610169491525|\n|    589|      221| 4.018099547511312|\n|    527|      218| 4.259174311926605|\n|   2959|      215| 4.325581395348837|\n|      1|      214|3.9369158878504673|\n|   1196|      210| 4.233333333333333|\n|     50|      204| 4.237745098039215|\n|   2858|      203| 4.073891625615763|\n|     47|      201| 4.009950248756219|\n|    150|      201| 3.845771144278607|\n|    780|      200|             3.475|\n|   1198|      199| 4.226130653266332|\n|   1210|      196| 4.137755102040816|\n+-------+---------+------------------+\nonly showing top 20 rows\n\n"
     ]
    }
   ],
   "source": [
    "import pyspark.sql.functions as F # F is alias name\n",
    "# F.col \n",
    "# groupBy will cause wider transformation, shuffle operation\n",
    "\n",
    "# print (\"ratingDf partitions \", ratingDf.rdd.getNumPartitions())\n",
    "\n",
    "# MISTAKE : SORTING DATA AT TOO EARLY, spark can optimize that for you.\n",
    "# sort is wider transformation, shuffle\n",
    "\n",
    "popularMoviesDf = ( ratingDf\n",
    "                        .groupBy(\"movieId\")\n",
    "                        .agg(\n",
    "                            F.count(\"userId\").alias(\"userCount\"),\n",
    "                            F.avg(\"rating\").alias(\"avgRating\")\n",
    "                        ) # agg\n",
    "                        .filter (F.col(\"userCount\") > 100)\n",
    "                        \n",
    "                        .sort (F.col(\"userCount\").desc())\n",
    "                    )\n",
    "\n",
    "# print (\"popularMoviesDf partitions \", popularMoviesDf.rdd.getNumPartitions())\n",
    "popularMoviesDf.printSchema()\n",
    "popularMoviesDf.show(20) # action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d726a7db-7d34-4cac-8297-168f3deb2637",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Physical Plan ==\nAdaptiveSparkPlan isFinalPlan=false\n+- == Initial Plan ==\n   Sort [userCount#3039L DESC NULLS LAST], true, 0\n   +- Exchange rangepartitioning(userCount#3039L DESC NULLS LAST, 200), ENSURE_REQUIREMENTS, [plan_id=3891]\n      +- Filter (userCount#3039L > 100)\n         +- HashAggregate(keys=[movieId#3002], functions=[finalmerge_count(merge count#3090L) AS count(userId#3001)#3084L, finalmerge_avg(merge sum#3093, count#3094L) AS avg(rating#3003)#3085])\n            +- Exchange hashpartitioning(movieId#3002, 200), ENSURE_REQUIREMENTS, [plan_id=3887]\n               +- HashAggregate(keys=[movieId#3002], functions=[partial_count(userId#3001) AS count#3090L, partial_avg(rating#3003) AS (sum#3093, count#3094L)])\n                  +- FileScan parquet [userId#3001,movieId#3002,rating#3003] Batched: true, DataFilters: [], Format: Parquet, Location: InMemoryFileIndex(1 paths)[abfss://silver@gksdatalake.dfs.core.windows.net/ratings], PartitionFilters: [], PushedFilters: [], ReadSchema: struct<userId:int,movieId:int,rating:double>\n\n\n"
     ]
    }
   ],
   "source": [
    "# runs on driver, not on worker\n",
    "popularMoviesDf.explain () # PHYSICAL PLAN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b562878b-5be3-42b0-b90f-e0c1b7519a6a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Parsed Logical Plan ==\n'Sort ['userCount DESC NULLS LAST], true\n+- 'Filter '`>`('userCount, 100)\n   +- 'Aggregate ['movieId], ['movieId, 'count('userId) AS userCount#3039, 'avg('rating) AS avgRating#3040]\n      +- Relation [userId#3001,movieId#3002,rating#3003,timestamp#3004L] parquet\n\n== Analyzed Logical Plan ==\nmovieId: int, userCount: bigint, avgRating: double\nSort [userCount#3039L DESC NULLS LAST], true\n+- Filter (userCount#3039L > cast(100 as bigint))\n   +- Aggregate [movieId#3002], [movieId#3002, count(userId#3001) AS userCount#3039L, avg(rating#3003) AS avgRating#3040]\n      +- Relation [userId#3001,movieId#3002,rating#3003,timestamp#3004L] parquet\n\n== Optimized Logical Plan ==\nSort [userCount#3039L DESC NULLS LAST], true\n+- Filter (userCount#3039L > 100)\n   +- Aggregate [movieId#3002], [movieId#3002, count(userId#3001) AS userCount#3039L, avg(rating#3003) AS avgRating#3040]\n      +- Project [userId#3001, movieId#3002, rating#3003]\n         +- Relation [userId#3001,movieId#3002,rating#3003,timestamp#3004L] parquet\n\n== Physical Plan ==\nAdaptiveSparkPlan isFinalPlan=false\n+- == Initial Plan ==\n   Sort [userCount#3039L DESC NULLS LAST], true, 0\n   +- Exchange rangepartitioning(userCount#3039L DESC NULLS LAST, 200), ENSURE_REQUIREMENTS, [plan_id=3929]\n      +- Filter (userCount#3039L > 100)\n         +- HashAggregate(keys=[movieId#3002], functions=[finalmerge_count(merge count#3111L) AS count(userId#3001)#3105L, finalmerge_avg(merge sum#3114, count#3115L) AS avg(rating#3003)#3106], output=[movieId#3002, userCount#3039L, avgRating#3040])\n            +- Exchange hashpartitioning(movieId#3002, 200), ENSURE_REQUIREMENTS, [plan_id=3925]\n               +- HashAggregate(keys=[movieId#3002], functions=[partial_count(userId#3001) AS count#3111L, partial_avg(rating#3003) AS (sum#3114, count#3115L)], output=[movieId#3002, count#3111L, sum#3114, count#3115L])\n                  +- FileScan parquet [userId#3001,movieId#3002,rating#3003] Batched: true, DataFilters: [], Format: Parquet, Location: InMemoryFileIndex(1 paths)[abfss://silver@gksdatalake.dfs.core.windows.net/ratings], PartitionFilters: [], PushedFilters: [], ReadSchema: struct<userId:int,movieId:int,rating:double>\n\n"
     ]
    }
   ],
   "source": [
    "popularMoviesDf.explain (extended= True ) # Print all plans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1b8dd886-377f-4a97-9d9e-42a3070c8f07",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n |-- movieId: integer (nullable = true)\n |-- title: string (nullable = true)\n |-- userCount: long (nullable = false)\n |-- avgRating: double (nullable = true)\n\n+-------+--------------------+---------+-----------------+\n|movieId|               title|userCount|        avgRating|\n+-------+--------------------+---------+-----------------+\n|    356| Forrest Gump (1994)|      328|4.175304878048781|\n|    318|Shawshank Redempt...|      317|4.429022082018927|\n|    296| Pulp Fiction (1994)|      305|4.221311475409836|\n|    593|Silence of the La...|      276|4.201086956521739|\n|   2571|  Matrix, The (1999)|      273| 4.26007326007326|\n|    260|Star Wars: Episod...|      250|            4.246|\n|    110|   Braveheart (1995)|      236|4.046610169491525|\n|    589|Terminator 2: Jud...|      221|4.018099547511312|\n|    527|Schindler's List ...|      218|4.259174311926605|\n|   2959|   Fight Club (1999)|      215|4.325581395348837|\n|   1196|Star Wars: Episod...|      210|4.233333333333333|\n|     50|Usual Suspects, T...|      204|4.237745098039215|\n|   2858|American Beauty (...|      203|4.073891625615763|\n|     47|Seven (a.k.a. Se7...|      201|4.009950248756219|\n|   1198|Raiders of the Lo...|      199|4.226130653266332|\n|   1210|Star Wars: Episod...|      196|4.137755102040816|\n|   4993|Lord of the Rings...|      196|4.142857142857143|\n|    858|Godfather, The (1...|      192|        4.2890625|\n|    457|Fugitive, The (1993)|      189| 4.01058201058201|\n|   5952|Lord of the Rings...|      187|4.040106951871658|\n+-------+--------------------+---------+-----------------+\nonly showing top 20 rows\n\n"
     ]
    }
   ],
   "source": [
    "# we have to to join with movieDf to know movie title\n",
    "# you may see that sort, what we have in previous cell is not needd, it was not respected here\n",
    "# join will shuffle data, sorted data again disordered.\n",
    "mostPopularMoviesDf =  popularMoviesDf.join (movieDf, popularMoviesDf.movieId == movieDf.movieId, \"inner\" )\\\n",
    "                                      .filter (F.col(\"avgRating\") > 4.0)\\\n",
    "                                      .sort (F.col(\"userCount\").desc())\\\n",
    "                                      .drop(popularMoviesDf.movieId)\\\n",
    "                                      .select(\"movieId\", \"title\", \"userCount\", \"avgRating\" )\n",
    "\n",
    "mostPopularMoviesDf.printSchema()\n",
    "mostPopularMoviesDf.show(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6b7b4bb1-9339-45f0-9db9-2b527a30bacf",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Parsed Logical Plan ==\n'Project ['movieId, 'title, 'userCount, 'avgRating]\n+- Project [userCount#3039L, avgRating#3040, movieId#2971, title#2972, genres#2973]\n   +- Sort [userCount#3039L DESC NULLS LAST], true\n      +- Filter (avgRating#3040 > 4.0)\n         +- Join Inner, (movieId#3002 = movieId#2971)\n            :- Sort [userCount#3039L DESC NULLS LAST], true\n            :  +- Filter (userCount#3039L > cast(100 as bigint))\n            :     +- Aggregate [movieId#3002], [movieId#3002, count(userId#3001) AS userCount#3039L, avg(rating#3003) AS avgRating#3040]\n            :        +- Relation [userId#3001,movieId#3002,rating#3003,timestamp#3004L] parquet\n            +- Relation [movieId#2971,title#2972,genres#2973] parquet\n\n== Analyzed Logical Plan ==\nmovieId: int, title: string, userCount: bigint, avgRating: double\nProject [movieId#2971, title#2972, userCount#3039L, avgRating#3040]\n+- Project [userCount#3039L, avgRating#3040, movieId#2971, title#2972, genres#2973]\n   +- Sort [userCount#3039L DESC NULLS LAST], true\n      +- Filter (avgRating#3040 > 4.0)\n         +- Join Inner, (movieId#3002 = movieId#2971)\n            :- Sort [userCount#3039L DESC NULLS LAST], true\n            :  +- Filter (userCount#3039L > cast(100 as bigint))\n            :     +- Aggregate [movieId#3002], [movieId#3002, count(userId#3001) AS userCount#3039L, avg(rating#3003) AS avgRating#3040]\n            :        +- Relation [userId#3001,movieId#3002,rating#3003,timestamp#3004L] parquet\n            +- Relation [movieId#2971,title#2972,genres#2973] parquet\n\n== Optimized Logical Plan ==\nProject [movieId#2971, title#2972, userCount#3039L, avgRating#3040]\n+- Sort [userCount#3039L DESC NULLS LAST], true\n   +- Project [userCount#3039L, avgRating#3040, movieId#2971, title#2972]\n      +- Join Inner, (movieId#3002 = movieId#2971)\n         :- Filter ((isnotnull(avgRating#3040) AND (userCount#3039L > 100)) AND (avgRating#3040 > 4.0))\n         :  +- Aggregate [movieId#3002], [movieId#3002, count(userId#3001) AS userCount#3039L, avg(rating#3003) AS avgRating#3040]\n         :     +- Project [userId#3001, movieId#3002, rating#3003]\n         :        +- Filter isnotnull(movieId#3002)\n         :           +- Relation [userId#3001,movieId#3002,rating#3003,timestamp#3004L] parquet\n         +- Project [movieId#2971, title#2972]\n            +- Filter isnotnull(movieId#2971)\n               +- Relation [movieId#2971,title#2972,genres#2973] parquet\n\n== Physical Plan ==\nAdaptiveSparkPlan isFinalPlan=false\n+- == Initial Plan ==\n   Project [movieId#2971, title#2972, userCount#3039L, avgRating#3040]\n   +- Sort [userCount#3039L DESC NULLS LAST], true, 0\n      +- Exchange rangepartitioning(userCount#3039L DESC NULLS LAST, 200), ENSURE_REQUIREMENTS, [plan_id=4163]\n         +- Project [userCount#3039L, avgRating#3040, movieId#2971, title#2972]\n            +- BroadcastHashJoin [movieId#3002], [movieId#2971], Inner, BuildRight, false, true\n               :- Filter ((isnotnull(avgRating#3040) AND (userCount#3039L > 100)) AND (avgRating#3040 > 4.0))\n               :  +- HashAggregate(keys=[movieId#3002], functions=[finalmerge_count(merge count#3161L) AS count(userId#3001)#3126L, finalmerge_avg(merge sum#3164, count#3165L) AS avg(rating#3003)#3127], output=[movieId#3002, userCount#3039L, avgRating#3040])\n               :     +- Exchange hashpartitioning(movieId#3002, 200), ENSURE_REQUIREMENTS, [plan_id=4155]\n               :        +- HashAggregate(keys=[movieId#3002], functions=[partial_count(userId#3001) AS count#3161L, partial_avg(rating#3003) AS (sum#3164, count#3165L)], output=[movieId#3002, count#3161L, sum#3164, count#3165L])\n               :           +- Filter isnotnull(movieId#3002)\n               :              +- FileScan parquet [userId#3001,movieId#3002,rating#3003] Batched: true, DataFilters: [isnotnull(movieId#3002)], Format: Parquet, Location: InMemoryFileIndex(1 paths)[abfss://silver@gksdatalake.dfs.core.windows.net/ratings], PartitionFilters: [], PushedFilters: [IsNotNull(movieId)], ReadSchema: struct<userId:int,movieId:int,rating:double>\n               +- Exchange SinglePartition, EXECUTOR_BROADCAST, [plan_id=4159]\n                  +- Filter isnotnull(movieId#2971)\n                     +- FileScan parquet [movieId#2971,title#2972] Batched: true, DataFilters: [isnotnull(movieId#2971)], Format: Parquet, Location: InMemoryFileIndex(1 paths)[abfss://silver@gksdatalake.dfs.core.windows.net/movies], PartitionFilters: [], PushedFilters: [IsNotNull(movieId)], ReadSchema: struct<movieId:int,title:string>\n\n"
     ]
    }
   ],
   "source": [
    "mostPopularMoviesDf.explain(extended = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "bec6b9d2-6bfe-4eaa-80df-0ae4f2a44b8e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mostPopularMoviesDf.is_cached # whether cached or not\n",
    "# DATA FRAME IS NOT CACHED\n",
    "# ASSUME WE DO Anotehr action\n",
    "# df.write # ACTION, read movies AGAIN , ratings AGAIN, aggreration AGAIN, join AGAIN\n",
    "# df.write # JOB AGAIN, STAGES AGAIN, TASKS/PARTITION AGAIN, SHUFFLEING AGAIN\n",
    "#mostPopularMoviesDf.write.mode(\"overwrite\").json('json-file-path')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e961e7f9-41b6-4175-a497-971e0c545fb4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# CAche will happen when we apply first action\n",
    "mostPopularMoviesDf.cache () # LAZY # MEMORY_AND_DISK\n",
    "from pyspark import StorageLevel\n",
    "#mostPopularMoviesDf.persist(StorageLevel.DISK_ONLY)\n",
    "mostPopularMoviesDf.is_cached # true"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6c545748-7fa1-4812-80c8-51869159560f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# now write the results to gold zone in parquet format\n",
    "POPULAR_MOVIES_TARGET_PATH = dbutils.widgets.get(\"popularMoviesPath\")\n",
    "# Wer GOT OUTPUT, write result to jdbc, json, orc, parquet, ...\n",
    "# df.write # ACTION, read movies, ratings, aggreration, join\n",
    "# df.write # JOB, STAGES, TASKS/PARTITION, SHUFFLEING\n",
    "mostPopularMoviesDf.coalesce(1).write.mode(\"overwrite\").parquet(POPULAR_MOVIES_TARGET_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7b6d166c-71d8-407d-b635-854c0fd4b900",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "MovieLens-Analytics",
   "widgets": {
    "moviesPath": {
     "currentValue": "abfss://silver@gksdatalake.dfs.core.windows.net/movies/",
     "nuid": "58237aea-654e-4a65-b396-3b609ae2fd57",
     "typedWidgetInfo": {
      "autoCreated": false,
      "defaultValue": "abfss://silver@gksdatalake.dfs.core.windows.net/movies/",
      "label": null,
      "name": "moviesPath",
      "options": {
       "widgetDisplayType": "Text",
       "validationRegex": null
      },
      "parameterDataType": "String"
     },
     "widgetInfo": {
      "widgetType": "text",
      "defaultValue": "abfss://silver@gksdatalake.dfs.core.windows.net/movies/",
      "label": null,
      "name": "moviesPath",
      "options": {
       "widgetType": "text",
       "autoCreated": false,
       "validationRegex": null
      }
     }
    },
    "popularMoviesPath": {
     "currentValue": "abfss://gold@gksdatalake.dfs.core.windows.net/popular-movies/",
     "nuid": "ec46d07e-961e-429d-a591-c729129ed27d",
     "typedWidgetInfo": {
      "autoCreated": false,
      "defaultValue": "abfss://gold@gksdatalake.dfs.core.windows.net/popular-movies/",
      "label": null,
      "name": "popularMoviesPath",
      "options": {
       "widgetDisplayType": "Text",
       "validationRegex": null
      },
      "parameterDataType": "String"
     },
     "widgetInfo": {
      "widgetType": "text",
      "defaultValue": "abfss://gold@gksdatalake.dfs.core.windows.net/popular-movies/",
      "label": null,
      "name": "popularMoviesPath",
      "options": {
       "widgetType": "text",
       "autoCreated": false,
       "validationRegex": null
      }
     }
    },
    "ratingsPath": {
     "currentValue": "abfss://silver@gksdatalake.dfs.core.windows.net/ratings/",
     "nuid": "439e787b-838a-483e-b258-4fd1f48044b2",
     "typedWidgetInfo": {
      "autoCreated": false,
      "defaultValue": "abfss://silver@gksdatalake.dfs.core.windows.net/ratings/",
      "label": null,
      "name": "ratingsPath",
      "options": {
       "widgetDisplayType": "Text",
       "validationRegex": null
      },
      "parameterDataType": "String"
     },
     "widgetInfo": {
      "widgetType": "text",
      "defaultValue": "abfss://silver@gksdatalake.dfs.core.windows.net/ratings/",
      "label": null,
      "name": "ratingsPath",
      "options": {
       "widgetType": "text",
       "autoCreated": false,
       "validationRegex": null
      }
     }
    }
   }
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}