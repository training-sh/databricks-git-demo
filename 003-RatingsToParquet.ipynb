{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "ee68c6f1-1dca-4eef-bd2e-4c99df3318eb",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# fixed issue reading blah blah...\n",
    "# fixed 2 of 5 issues"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "54bbeec1-b481-46fb-bd5f-50a6b013fdaf",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "dbutils.widgets.removeAll()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6126929f-1054-4753-b6de-e938a441efb0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Convert CSV files into PArquet format\n",
    "# Delta Lake: Move data from Bronze to Silver\n",
    "# Row based CSV foramt to Column based Parquet\n",
    "\n",
    "dbutils.widgets.text(\"ratingsPath\", \"abfss://bronze@gksdatalake.dfs.core.windows.net/ratings/\")\n",
    "dbutils.widgets.text(\"ratingsTargetPath\", \"abfss://silver@gksdatalake.dfs.core.windows.net/ratings/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3953759b-1edc-41cc-8576-a3821953b241",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "RATINGS_PATH = dbutils.widgets.get(\"ratingsPath\") \n",
    "print (RATINGS_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "16db9395-db70-470d-bda3-1e6bbd5a289e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# how to create schema programatically instead of using inferSchema\n",
    "from pyspark.sql.types import StructType, LongType, StringType, IntegerType, DoubleType"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "afcf58f2-2da4-4393-b2e8-419d28e0677f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# True is nullable, False is non nullable\n",
    "# give your own column name, datatypes\n",
    "ratingSchema = StructType()\\\n",
    "                .add(\"userId\", IntegerType(), True)\\\n",
    "                .add(\"movieId\", IntegerType(), True)\\\n",
    "                .add(\"rating\", DoubleType(), True)\\\n",
    "                .add(\"timestamp\", LongType(), True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4d503596-3f26-48d6-9cd1-5c562d319b2d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# movieDf with schema we have, to avoid additional job creation, scan data overload\n",
    "# since we provide schema, header = true, is to skip the first line in the csv\n",
    "ratingDf = (spark\n",
    "            .read\n",
    "            .option(\"header\", True)\n",
    "            .schema(ratingSchema) # now , we don't run a job for schema\n",
    "            .csv(RATINGS_PATH)\n",
    "            )\n",
    "ratingDf.printSchema()\n",
    "ratingDf.show(5)\n",
    "# make a note of number of jobs its create, compare with previous shell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "482d28ff-1e08-47b3-b7e8-466aab77a472",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# ratings data, convert to parquet as is\n",
    "# silver container\n",
    "# movies directory\n",
    "\n",
    "# ratings to silver container\n",
    " \n",
    "RATINGS_TARGET_PATH = dbutils.widgets.get(\"ratingsTargetPath\")\n",
    "# overwrite - delete old data\n",
    "# add a filter, if ratings <= 0.5, do not move to silver\n",
    "\n",
    "(ratingDf\n",
    "    .filter (\"rating > 0.5\")\n",
    "    .write\n",
    "    .mode(\"overwrite\")\n",
    "    .parquet(RATINGS_TARGET_PATH))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "db23309d-9382-41a0-b70b-e993bece8bae",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "dbutils.jobs.taskValues.set(key = \"ratings_silver_path\", value = RATINGS_TARGET_PATH)\n"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "2"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "003-RatingsToParquet",
   "widgets": {
    "ratingsPath": {
     "currentValue": "abfss://bronze@gksdatalake.dfs.core.windows.net/ratings/",
     "nuid": "65a694b3-0f4a-4e95-be8e-68945c0b92db",
     "typedWidgetInfo": {
      "autoCreated": false,
      "defaultValue": "abfss://bronze@gksdatalake.dfs.core.windows.net/ratings/",
      "label": null,
      "name": "ratingsPath",
      "options": {
       "widgetDisplayType": "Text",
       "validationRegex": null
      },
      "parameterDataType": "String"
     },
     "widgetInfo": {
      "widgetType": "text",
      "defaultValue": "abfss://bronze@gksdatalake.dfs.core.windows.net/ratings/",
      "label": null,
      "name": "ratingsPath",
      "options": {
       "widgetType": "text",
       "autoCreated": false,
       "validationRegex": null
      }
     }
    },
    "ratingsTargetPath": {
     "currentValue": "abfss://silver@gksdatalake.dfs.core.windows.net/ratings/",
     "nuid": "864579f4-e23f-4604-ae74-1ef5cade88a6",
     "typedWidgetInfo": {
      "autoCreated": false,
      "defaultValue": "abfss://silver@gksdatalake.dfs.core.windows.net/ratings/",
      "label": null,
      "name": "ratingsTargetPath",
      "options": {
       "widgetDisplayType": "Text",
       "validationRegex": null
      },
      "parameterDataType": "String"
     },
     "widgetInfo": {
      "widgetType": "text",
      "defaultValue": "abfss://silver@gksdatalake.dfs.core.windows.net/ratings/",
      "label": null,
      "name": "ratingsTargetPath",
      "options": {
       "widgetType": "text",
       "autoCreated": false,
       "validationRegex": null
      }
     }
    }
   }
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
