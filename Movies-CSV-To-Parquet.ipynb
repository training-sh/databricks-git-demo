{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a80c19be-759a-405e-922b-bc9b8fa2cad0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "name Gopal\n"
     ]
    }
   ],
   "source": [
    "dbutils.widgets.text(\"myname\", \"Gopal\")\n",
    "name = dbutils.widgets.get(\"myname\")  # read data from widget\n",
    "print (\"name\", name)\n",
    "\n",
    "dbutils.widgets.remove(\"myname\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3f4a9f20-c8f9-4487-93c0-9bb552fbfafb",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "dbutils.widgets.removeAll()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f25bf90b-afad-4690-8765-2f2bd404a96e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Convert CSV files into PArquet format\n",
    "# Delta Lake: Move data from Bronze to Silver\n",
    "# Row based CSV foramt to Column based Parquet\n",
    "\n",
    "\n",
    "dbutils.widgets.text(\"moviesPath\", \"abfss://bronze@gksdatalake.dfs.core.windows.net/movies/\")\n",
    "dbutils.widgets.text(\"moviesTargetPath\", \"abfss://silver@gksdatalake.dfs.core.windows.net/movies/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "15eec575-d572-4c69-aa08-434067cffe81",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "abfss://bronze@gksdatalake.dfs.core.windows.net/movies/\n"
     ]
    }
   ],
   "source": [
    "MOVIES_PATH = dbutils.widgets.get(\"moviesPath\") \n",
    "print (MOVIES_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "78026d03-b96e-42ac-a099-eb771fc592cd",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "<pyspark.sql.connect.session.SparkSession at 0x7f917c733290>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Spark Session, entry point for spark sql\n",
    "spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "18f434c8-9546-47bc-8210-043275a2f927",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n |-- movieId: string (nullable = true)\n |-- title: string (nullable = true)\n |-- genres: string (nullable = true)\n\n+-------+--------------------+--------------------+\n|movieId|               title|              genres|\n+-------+--------------------+--------------------+\n|      1|    Toy Story (1995)|Adventure|Animati...|\n|      2|      Jumanji (1995)|Adventure|Childre...|\n|      3|Grumpier Old Men ...|      Comedy|Romance|\n|      4|Waiting to Exhale...|Comedy|Drama|Romance|\n|      5|Father of the Bri...|              Comedy|\n+-------+--------------------+--------------------+\nonly showing top 5 rows\n\n"
     ]
    }
   ],
   "source": [
    "# read movies data\n",
    "# spark.read is BATCH PROCESSING\n",
    "# read is LAZY EVAL, TRANSFORMATION\n",
    "moviesDf = spark.read.option(\"header\", True).csv(MOVIES_PATH)\n",
    "moviesDf.printSchema() # used Job 1\n",
    "moviesDf.show(5) # ACTION, Job 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "526ab31a-6b3b-4933-82d6-60bfff01c1c5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n |-- movieId: integer (nullable = true)\n |-- title: string (nullable = true)\n |-- genres: string (nullable = true)\n\n+-------+--------------------+--------------------+\n|movieId|               title|              genres|\n+-------+--------------------+--------------------+\n|      1|    Toy Story (1995)|Adventure|Animati...|\n|      2|      Jumanji (1995)|Adventure|Childre...|\n|      3|Grumpier Old Men ...|      Comedy|Romance|\n|      4|Waiting to Exhale...|Comedy|Drama|Romance|\n|      5|Father of the Bri...|              Comedy|\n+-------+--------------------+--------------------+\nonly showing top 5 rows\n\n"
     ]
    }
   ],
   "source": [
    "# inferSchema - not recommended, let spark to build a schema by scanning data\n",
    "# if you use inferSchema, use the sampleRatio with %, it won't scan all data\n",
    "# sampleRatio = 0.01, it will scan 1% of data, 0.1 means 10% of data\n",
    "# inferSchema with all data scan can be very expensive, assume your input is 2 TB size\n",
    "# the read data woun't be useful in later action. EVERY ACTION IS INDEPENENT\n",
    "# uncomment CTRL + /\n",
    "\n",
    "movieDf = spark.read\\\n",
    "                .option(\"header\", True)\\\n",
    "                .option (\"inferSchema\", \"true\")\\\n",
    "                .option (\"samplingRatio\", 0.01)\\\n",
    "                .csv(MOVIES_PATH)\n",
    "\n",
    "# since we have not given SCHEMA, printSchema execute action for column name and datatypes\n",
    "\n",
    "movieDf.printSchema()\n",
    "\n",
    "movieDf.show(5) # ACTION function, create a job\n",
    "\n",
    "# watch out, without header, with header, with inferSchema, without inferSchema\n",
    "# with header, without schema, with inferSchema = 3 jobs\n",
    "#     job 1 - to obtain column names from csv\n",
    "#     job 2 - to infer schema, that scan the data to find the datatypes for columns\n",
    "#     job 3 - for .show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f08c589d-08ee-44f3-9912-1b32eb4b63b2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# how to create schema programatically instead of using inferSchema\n",
    "from pyspark.sql.types import StructType, LongType, StringType, IntegerType, DoubleType"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "bbe6ffd5-46d3-4b3b-90a1-1da90e988e10",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# True is nullable, False is non nullable\n",
    "# give your own column name, datatypes\n",
    "movieSchema = StructType()\\\n",
    "                .add(\"movieId\", IntegerType(), True)\\\n",
    "                .add(\"title\", StringType(), True)\\\n",
    "                .add(\"genres\", StringType(), True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "eb8e236a-7c35-472e-9c6c-e0d086ccb022",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n |-- movieId: integer (nullable = true)\n |-- title: string (nullable = true)\n |-- genres: string (nullable = true)\n\n+-------+--------------------+--------------------+\n|movieId|               title|              genres|\n+-------+--------------------+--------------------+\n|      1|    Toy Story (1995)|Adventure|Animati...|\n|      2|      Jumanji (1995)|Adventure|Childre...|\n|      3|Grumpier Old Men ...|      Comedy|Romance|\n|      4|Waiting to Exhale...|Comedy|Drama|Romance|\n|      5|Father of the Bri...|              Comedy|\n+-------+--------------------+--------------------+\nonly showing top 5 rows\n\n"
     ]
    }
   ],
   "source": [
    "# movieDf with schema we have, to avoid additional job creation, scan data overload\n",
    "# since we provide schema, header = true, is to skip the first line in the csv\n",
    "# spark.conf.set(\"fs.azure.account.key.ugdatalake4.dfs.core.windows.net\", \"<your-storage-account-key>\")\n",
    "# spark\\\n",
    "#     .read\\\n",
    "#     .option(\"header\", True)\\\n",
    "#     .schema(movieSchema)\\\n",
    "#     .csv(MOVIES_PATH)\\\n",
    "#     .show(5)\n",
    "\n",
    "movieDf = (spark\n",
    "            .read\n",
    "            .option(\"header\", True)\n",
    "            .schema(movieSchema) # now , we don't run a job for schema\n",
    "            .csv(MOVIES_PATH)\n",
    "            )\n",
    "\n",
    "movieDf.printSchema()\n",
    "movieDf.show(5)\n",
    "# make a note of number of jobs its create, compare with previous shell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2323c627-ec92-42c8-9591-b70df65ff5dc",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# movie data, convert to parquet as is\n",
    "# silver container\n",
    "# movies directory\n",
    "\n",
    "# move to silver container\n",
    "# Put \n",
    "# Put MOVIE_TARGET_PATH as part of the widget\n",
    "\n",
    "MOVIE_TARGET_PATH = dbutils.widgets.get(\"moviesTargetPath\")\n",
    "# overwrite - delete old data\n",
    "# write - ACTION - trigger a job\n",
    "movieDf.write.mode(\"overwrite\").parquet(MOVIE_TARGET_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e26cdcee-d5e3-413c-8cc9-c2f3a36ad286",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "manifest = {\n",
    "    \"inputCsvPath\": MOVIES_PATH,\n",
    "    \"resultParquetPath\": MOVIE_TARGET_PATH,\n",
    "    \"result\": \"SUCCESS\"\n",
    "}\n",
    "\n",
    "print (json.dumps(manifest))\n",
    "\n",
    "dbutils.notebook.exit(json.dumps(manifest))"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "Movies-CSV-To-Parquet",
   "widgets": {
    "moviesPath": {
     "currentValue": "abfss://bronze@gksdatalake.dfs.core.windows.net/movies/",
     "nuid": "cf947bc0-9b61-4b3f-b9d3-2ea601594cfc",
     "typedWidgetInfo": {
      "autoCreated": false,
      "defaultValue": "abfss://bronze@gksdatalake.dfs.core.windows.net/movies/",
      "label": null,
      "name": "moviesPath",
      "options": {
       "widgetDisplayType": "Text",
       "validationRegex": null
      },
      "parameterDataType": "String"
     },
     "widgetInfo": {
      "widgetType": "text",
      "defaultValue": "abfss://bronze@gksdatalake.dfs.core.windows.net/movies/",
      "label": null,
      "name": "moviesPath",
      "options": {
       "widgetType": "text",
       "autoCreated": false,
       "validationRegex": null
      }
     }
    },
    "moviesTargetPath": {
     "currentValue": "abfss://silver@gksdatalake.dfs.core.windows.net/movies/",
     "nuid": "9a55eaf8-a3c0-40fd-aff8-73d265aca004",
     "typedWidgetInfo": {
      "autoCreated": false,
      "defaultValue": "abfss://silver@gksdatalake.dfs.core.windows.net/movies/",
      "label": null,
      "name": "moviesTargetPath",
      "options": {
       "widgetDisplayType": "Text",
       "validationRegex": null
      },
      "parameterDataType": "String"
     },
     "widgetInfo": {
      "widgetType": "text",
      "defaultValue": "abfss://silver@gksdatalake.dfs.core.windows.net/movies/",
      "label": null,
      "name": "moviesTargetPath",
      "options": {
       "widgetType": "text",
       "autoCreated": false,
       "validationRegex": null
      }
     }
    }
   }
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}